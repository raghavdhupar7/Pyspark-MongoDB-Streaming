{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2040a8da-bcbb-4880-a1b4-cb256e27e950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dbutils\n  Downloading DBUtils-3.0.3-py3-none-any.whl (32 kB)\nInstalling collected packages: dbutils\nSuccessfully installed dbutils-3.0.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc60ff7b-2e8c-4a70-b21f-3f190acb50b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting pymongo\n  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 677.1/677.1 kB 7.2 MB/s eta 0:00:00\nCollecting dnspython<3.0.0,>=1.16.0\n  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.4/300.4 kB 39.6 MB/s eta 0:00:00\nInstalling collected packages: dnspython, pymongo\nSuccessfully installed dnspython-2.4.2 pymongo-4.6.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e21d468-6d08-4f09-b452-d8a90da8dc03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n3077\nReceiving the data...\nChange details: No update description\nFull Document: {'_id': ObjectId('65781f15cfd6a69a4395fceb'), 'car_id': '5c57e09175e12b9912fb7790ae77cd4c', 'car_make': 'MARUTI', 'car_model': '800', 'car_gears': 5, 'car_fuel_type': 'Petrol', 'car_transmission_type': 'Manual'}\nReceiving the data...\nChange details: No update description\nFull Document: {'_id': ObjectId('659bd637c1edeb7c3c6e6477'), 'car_id': '5c57e09175e90b5512fb7790ae77cd4c', 'car_make': 'MAZDA', 'car_model': '1000', 'car_gears': 5, 'car_fuel_type': 'Unknown', 'car_transmission_type': 'Manual'}\nflag: 39\nflag: 38\nflag: 37\nflag: 36\nflag: 35\nflag: 34\nflag: 33\nflag: 32\nflag: 31\nflag: 30\nflag: 29\nflag: 28\nflag: 27\nflag: 26\nflag: 25\nflag: 24\nflag: 23\nflag: 22\nflag: 21\nflag: 20\nflag: 19\nflag: 18\nflag: 17\nflag: 16\nflag: 15\nflag: 14\nflag: 13\nflag: 12\nflag: 11\nflag: 10\nflag: 9\nflag: 8\nflag: 7\nflag: 6\nflag: 5\nflag: 4\nflag: 3\nflag: 2\nflag: 1\nflag: 0\nlen== 2\n{\"car_id\": \"5c57e09175e12b9912fb7790ae77cd4c\", \"car_make\": \"MARUTI\", \"car_model\": \"800\", \"car_gears\": 5, \"car_fuel_type\": \"Petrol\", \"car_transmission_type\": \"Manual\", \"hive_audit_create_ts\": \"2023-12-12 08:51:33.000000\", \"hive_audit_update_ts\": \"2024-01-10 07:28:13.273071\", \"is_deleted\": \"false\", \"Policy_Data_car_id\": \"65781f15cfd6a69a4395fceb\"}\n\n{\"car_id\": \"5c57e09175e90b5512fb7790ae77cd4c\", \"car_make\": \"MAZDA\", \"car_model\": \"1000\", \"car_gears\": 5, \"car_fuel_type\": \"Unknown\", \"car_transmission_type\": \"Manual\", \"hive_audit_create_ts\": \"2024-01-08 11:02:15.000000\", \"hive_audit_update_ts\": \"2024-01-10 07:28:13.273071\", \"is_deleted\": \"false\", \"Policy_Data_car_id\": \"659bd637c1edeb7c3c6e6477\"}\n\nTrue\nWrote 697 bytes.\n{'status': 'BATCH_IMDT', 'mongo_count': 3077, 'resume_token': '82659E4749000000082B0229296E04', 'end_time': '2024-01-10 07:28:13.273071', 'event_details': {'data_file': '/dbfs/FileStore/files/car_2024011007.json', 'data_file_temp': '/dbfs/FileStore/files/_temp/1/car_2024011007.json', 'sequence': 1, 'record_length': 2, 'file_size': 697}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import time\n",
    "import pymongo\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "import sys\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from bson.json_util import dumps\n",
    "from urllib.parse import quote_plus\n",
    "import uuid\n",
    "import bson\n",
    "import logging as lambdaLogger\n",
    "\n",
    "Date_format = '%Y-%m-%d %H:%M:%S.%f'\n",
    "today = datetime.today().strftime(Date_format)\n",
    "raw = \"raw/\"\n",
    "\n",
    "#uploading to S3 when S3 is setup\n",
    "def upload_file_to_s3(bucketname, filePathInS3, localFilePath, region=dict['REGION']):\n",
    "\n",
    "    \"\"\"\n",
    "    Uploads the local file to S3\n",
    "    :param bucketname: Name of the bucket\n",
    "    :param filePathInS3: Path of file in S3\n",
    "    :param localFilePath: Path of file in local\n",
    "    :param region: AWS region\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    resource = boto3.resource(service_name='s3', region_name=region)\n",
    "    resource.Bucket(bucketname).upload_file(localFilePath, filePathInS3,\n",
    "\n",
    "                                            ExtraArgs={'ServerSideEncryption': 'aws:kms',\n",
    "                                                       'SSEKMSKeyId': dict[\"S3_ENCRYPTION_KEY\"]})\n",
    "    return localFilePath\n",
    "\n",
    "\n",
    "#uploading to DBFS because s3 is not there\n",
    "def upload_file_to_dbfs(local_file_path,dbfs_path):\n",
    "    \"\"\"\n",
    "    Uploads the local file to the specified DBFS path.\n",
    "\n",
    "    :param dbfs_path: The full path to the destination file on DBFS (e.g., \"dbfs:/FileStore/files/my_file.txt\")\n",
    "    :param local_file_path: The path to the local file to upload\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.cp(local_file_path, dbfs_path)\n",
    "        print(f\"File uploaded to DBFS: {dbfs_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "\n",
    "#main function\n",
    "connectUrl = \"mongodb+srv://aryandeepak2025:xxxxxxxxxx@cluster0.bgm1ffu.mongodb.net/\"\n",
    "try:\n",
    "    client = MongoClient(connectUrl)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"Connected Successfully!\")\n",
    "schema = \"Policy_Data\"\n",
    "coll = \"car\"\n",
    "database = client[schema]\n",
    "coll_client = database[coll]\n",
    "\n",
    "cnt=coll_client.count_documents({})\n",
    "cnt = coll_client.count_documents({\"_id\": {\"$exists\": True}})\n",
    "print(cnt)\n",
    "\n",
    "document_collection = {}\n",
    "upsert_list = []\n",
    "\n",
    "flag=40\n",
    "res_token = '82659BB888000000072B0229296E04'\n",
    "#res_token = '82659E4688000000072B0229296E04'\n",
    "#res_token = None\n",
    "resume_token = {}\n",
    "if res_token == None:\n",
    "    resume_token = None\n",
    "else:\n",
    "    resume_token['_data'] = res_token\n",
    "lambdaLogger.debug(\"### This is resume Token \\n\")\n",
    "lambdaLogger.debug(resume_token)\n",
    "            \n",
    "with coll_client.watch(resume_after=resume_token, full_document=\"updateLookup\") if resume_token is not None else coll_client.watch(full_document=\"updateLookup\") as stream:\n",
    "    while stream.alive:\n",
    "        change = stream.try_next()\n",
    "        #print(\"Change details:\", change)\n",
    "        resume_token = stream.resume_token  # Update the resume_token variable\n",
    "        #print(coll_client.list_indexes())\n",
    "        #print(resume_token)\n",
    "        if change is not None:\n",
    "            print(\"Receiving the data...\")\n",
    "            print(\"Change details:\", change.get('updateDescription', 'No update description'))\n",
    "            print(\"Full Document:\", change.get('fullDocument', 'No full document'))\n",
    "            if \"fullDocument\" in change and change[\"fullDocument\"] is not None:\n",
    "                oid_temp=change[\"fullDocument\"].get(\"_id\")\n",
    "                if(oid_temp != {} and type(oid_temp)==bson.objectid.ObjectId):\n",
    "                    create_dt = change[\"fullDocument\"].get(\"_id\").generation_time.strftime(Date_format)\n",
    "                else:\n",
    "                    print(change[\"fullDocument\"])\n",
    "                    create_dt='NA'\n",
    "                    \n",
    "                temp_data = change[\"fullDocument\"]\n",
    "                temp_data[\"hive_audit_create_ts\"] = create_dt\n",
    "                temp_data[\"hive_audit_update_ts\"] = today\n",
    "                temp_data[\"is_deleted\"] = \"false\"\n",
    "                temp_data[schema+\"_\"+coll+\"_id\"] = str(oid_temp)\n",
    "                temp_data.pop('_id')\n",
    "                upsert_list.append(dumps(temp_data))\n",
    "            else:\n",
    "                oid_temp=change[\"documentKey\"].get(\"_id\")\n",
    "                if(oid_temp != {} and type(oid_temp)==bson.objectid.ObjectId):\n",
    "                    create_dt = change[\"documentKey\"].get(\"_id\").generation_time.strftime(Date_format)\n",
    "                else:\n",
    "                    print(change[\"documentKey\"])\n",
    "                    create_dt='NA'\n",
    "                temp_data = change[\"documentKey\"]\n",
    "                temp_data[\"hive_audit_create_ts\"] = create_dt\n",
    "                temp_data[\"hive_audit_update_ts\"] = today\n",
    "                temp_data[\"is_deleted\"] = \"true\"\n",
    "                temp_data.pop('_id')\n",
    "                upsert_list.append(dumps(temp_data))\n",
    "        else:\n",
    "            flag=flag-1\n",
    "            print(\"flag:\",flag)\n",
    "            if(flag<=0):\n",
    "                break\n",
    "    batch_id = '1'\n",
    "    print(\"len==\", len(upsert_list))\n",
    "    current_date = str(datetime.now())\n",
    "    current_date = str(current_date[:4]) + str(current_date[5:7]) + str(current_date[8:10]) + str(current_date[11:13])\n",
    "    data_file_location = \"/dbfs/FileStore/files\"\n",
    "    data_file_location = data_file_location[:-1] if data_file_location[-1] == '/' else data_file_location\n",
    "\n",
    "    data_file_temp = str(data_file_location)  + '/' + '_temp/' + batch_id + '/' + coll + '_' + current_date + '.json'\n",
    "\n",
    "    data_file_temp2 = '{}_temp/{}/{}_{}.data'.format(str(data_file_location),batch_id,coll,current_date)\n",
    "\n",
    "    #data_file =  str(data_file_location) + '/' + coll + '_' + today + '.data'\n",
    "    data_file =  str(data_file_location) + '/' + coll + '_' + current_date + '.json'\n",
    "\n",
    "    if len(upsert_list) == 0:\n",
    "        print(\"No items found\")\n",
    "        meta_details = {'data_file': data_file, 'data_file_temp': data_file_temp, 'sequence': 0, \\\n",
    "                        'record_length': 0, 'file_size': 0}\n",
    "    else:\n",
    "        # Collecting meta information\n",
    "        # Collecting meta information\n",
    "        record_length = len(upsert_list)\n",
    "        sequence = 1\n",
    "        upsert_file_name = f\"/tmp/{coll}\"  # Use f-string for clarity\n",
    "        data = \"\"\n",
    "        with open(upsert_file_name, 'w') as file:\n",
    "            for l in upsert_list:\n",
    "                file.write(f\"{l}\\n\")  # Ensure newline is added\n",
    "                print(f\"{l}\\n\")\n",
    "                data_to_write = l\n",
    "                data += l + \",\"\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        file_size = os.path.getsize(upsert_file_name)\n",
    "        print(os.path.isfile(upsert_file_name))\n",
    "        #print(file_size)\n",
    "        meta_details = {\n",
    "            'data_file': data_file,\n",
    "            'data_file_temp': data_file_temp,\n",
    "            'sequence': sequence,\n",
    "            'record_length': record_length,\n",
    "            'file_size': file_size\n",
    "        }\n",
    "\n",
    "        # Uploading file to DBFS\n",
    "        \n",
    "        dbfs_destination = f\"{data_file}\"  # Use coll for filename\n",
    "\n",
    "        #dbutils.fs.rm(f\"dbfs:/FileStore/files/{data_file}\")\n",
    "        #dbutils.fs.put(f\"dbfs:/FileStore/files/{data_file}\", data_to_write)\n",
    "        dbutils.fs.rm(\"dbfs:/FileStore/files/direct_write.json\")\n",
    "        dbutils.fs.put(\"dbfs:/FileStore/files/direct_write.json\", data)\n",
    "        #dbutils.fs.put(dbfs_destination, upsert_file_name, overwrite=True)\n",
    "        #upload_file_to_dbfs(upsert_file_name, dbfs_destination)  # Correct function call order\n",
    "        #df.write.format(\"json\").save(dbfs_destination)  # Replace \"csv\" with desired format\n",
    "        #dbutils.fs.put(\"/dbfs/FileStore/files\", upsert_file_name)\n",
    "        \n",
    "        update_data = {\n",
    "            'status': \"BATCH_IMDT\",\n",
    "            'mongo_count': cnt,\n",
    "            'resume_token': resume_token['_data'],\n",
    "            'end_time': today,\n",
    "            'event_details': meta_details\n",
    "        }\n",
    "        print(update_data)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e607cbf-490b-42e8-bbe6-70e63d7065ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/files/car.csv</td><td>car.csv</td><td>8</td><td>1704706881000</td></tr><tr><td>dbfs:/FileStore/files/car_2024-01-08 09:13:40.004783.data</td><td>car_2024-01-08 09:13:40.004783.data</td><td>8</td><td>1704707305000</td></tr><tr><td>dbfs:/FileStore/files/car_2024-01-08 09:55:58.991827.data</td><td>car_2024-01-08 09:55:58.991827.data</td><td>8</td><td>1704707810000</td></tr><tr><td>dbfs:/FileStore/files/car_2024010809.data</td><td>car_2024010809.data</td><td>8</td><td>1704707905000</td></tr><tr><td>dbfs:/FileStore/files/car_2024010810.data</td><td>car_2024010810.data</td><td>8</td><td>1704710628000</td></tr><tr><td>dbfs:/FileStore/files/claim_1.json</td><td>claim_1.json</td><td>588675</td><td>1702987077000</td></tr><tr><td>dbfs:/FileStore/files/claim_2.json</td><td>claim_2.json</td><td>588675</td><td>1702987077000</td></tr><tr><td>dbfs:/FileStore/files/claim_3.json</td><td>claim_3.json</td><td>588675</td><td>1702987078000</td></tr><tr><td>dbfs:/FileStore/files/claim_4.json</td><td>claim_4.json</td><td>588675</td><td>1702987078000</td></tr><tr><td>dbfs:/FileStore/files/control.csv</td><td>control.csv</td><td>74</td><td>1702987079000</td></tr><tr><td>dbfs:/FileStore/files/dbfs/</td><td>dbfs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/files/dbfs:/</td><td>dbfs:/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/files/direct_write.json</td><td>direct_write.json</td><td>697</td><td>1704871755000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/files/car.csv",
         "car.csv",
         8,
         1704706881000
        ],
        [
         "dbfs:/FileStore/files/car_2024-01-08 09:13:40.004783.data",
         "car_2024-01-08 09:13:40.004783.data",
         8,
         1704707305000
        ],
        [
         "dbfs:/FileStore/files/car_2024-01-08 09:55:58.991827.data",
         "car_2024-01-08 09:55:58.991827.data",
         8,
         1704707810000
        ],
        [
         "dbfs:/FileStore/files/car_2024010809.data",
         "car_2024010809.data",
         8,
         1704707905000
        ],
        [
         "dbfs:/FileStore/files/car_2024010810.data",
         "car_2024010810.data",
         8,
         1704710628000
        ],
        [
         "dbfs:/FileStore/files/claim_1.json",
         "claim_1.json",
         588675,
         1702987077000
        ],
        [
         "dbfs:/FileStore/files/claim_2.json",
         "claim_2.json",
         588675,
         1702987077000
        ],
        [
         "dbfs:/FileStore/files/claim_3.json",
         "claim_3.json",
         588675,
         1702987078000
        ],
        [
         "dbfs:/FileStore/files/claim_4.json",
         "claim_4.json",
         588675,
         1702987078000
        ],
        [
         "dbfs:/FileStore/files/control.csv",
         "control.csv",
         74,
         1702987079000
        ],
        [
         "dbfs:/FileStore/files/dbfs/",
         "dbfs/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/files/dbfs:/",
         "dbfs:/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/files/direct_write.json",
         "direct_write.json",
         697,
         1704871755000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_file_location = \"dbfs:/FileStore/files\"\n",
    "display(dbutils.fs.ls(data_file_location))  # Check if the directory exists\n",
    "#dbutils.fs.ls(\"dbfs:/FileStore/files/car_2024010811.data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979afd52-b9b0-4637-90f5-198ee46c1ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"car_id\": \"5c57e09175e12b9912fb7790ae77cd4c\", \"car_make\": \"MARUTI\", \"car_model\": \"800\", \"car_gears\": 5, \"car_fuel_type\": \"Petrol\", \"car_transmission_type\": \"Manual\", \"hive_audit_create_ts\": \"2023-12-12 08:51:33.000000\", \"hive_audit_update_ts\": \"2024-01-10 07:28:13.273071\", \"is_deleted\": \"false\", \"Policy_Data_car_id\": \"65781f15cfd6a69a4395fceb\"},{\"car_id\": \"5c57e09175e90b5512fb7790ae77cd4c\", \"car_make\": \"MAZDA\", \"car_model\": \"1000\", \"car_gears\": 5, \"car_fuel_type\": \"Unknown\", \"car_transmission_type\": \"Manual\", \"hive_audit_create_ts\": \"2024-01-08 11:02:15.000000\", \"hive_audit_update_ts\": \"2024-01-10 07:28:13.273071\", \"is_deleted\": \"false\", \"Policy_Data_car_id\": \"659bd637c1edeb7c3c6e6477\"},'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"dbfs:/FileStore/files/direct_write.json\"\n",
    "\n",
    "# Read the file using Databricks utilities\n",
    "dbutils.fs.head(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359e40b2-432e-48ed-8e0f-19256f7c7d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Incremental working",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
